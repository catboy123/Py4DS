# -*- coding: utf-8 -*-
"""Py4DS_Lab6_Ex4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JNuXvegBid4Pd_TevIHFcdJbc3v7AzCp
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt 
import re
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")

def Constant_Features(x_train, x_test,threshold=0):
  """
  Removing Constant Features using Variance Threshold
  Input: threshold parameter to identify the variable as constant
         train data (pd.Dataframe) 
         test data (pd.Dataframe)
  Output: train data, test data after applying filter methods
  """
  # import and create the VarianceThreshold object.
  from sklearn.feature_selection import VarianceThreshold
  vs_constant = VarianceThreshold(threshold)

  # select the numerical columns only.
  numerical_x_train = x_train[x_train.select_dtypes([np.number]).columns]

  # fit the object to our data.
  vs_constant.fit(numerical_x_train)

  # get the constant colum names.
  constant_columns = [column for column in numerical_x_train.columns
                      if column not in numerical_x_train.columns[vs_constant.get_support()]]

  # detect constant categorical variables.
  constant_cat_columns = [column for column in x_train.columns 
                          if (x_train[column].dtype == "O" and len(x_train[column].unique())  == 1 )]

  # concatenating the two lists.
  all_constant_columns = constant_cat_columns + constant_columns
  
  return all_constant_columns

def Quansi_Constant_Feature(x_train, x_test,threshold=0.98):
  # create empty list
  quasi_constant_feature = []

  # loop over all the columns
  for feature in x_train.columns:
    # calculate the ratio.
    predominant = (x_train[feature].value_counts() / np.float(len(x_train))).sort_values(ascending=False).values[0]
    
    # append the column name if it is bigger than the threshold
    if predominant >= threshold:
        quasi_constant_feature.append(feature)   
  return quasi_constant_feature

def Dupplicate_Feature(x_train,x_test):
  # transpose the feature matrice
  train_features_T = x_train.T

  # print the number of duplicated features
  print(train_features_T.duplicated().sum())

  # select the duplicated features columns names
  duplicated_columns = train_features_T[train_features_T.duplicated()].index.values

  return duplicated_columns

def Correlated_Feature(x_train,x_test,threshold=0.8):
  # creating set to hold the correlated features
  corr_features = set()

  # create the correlation matrix (default to pearson)
  corr_matrix = x_train.corr()
  '''
  # optional: display a heatmap of the correlation matrix
  plt.figure(figsize=(11,11))
  sns.heatmap(corr_matrix)
  #'''

  for i in range(len(corr_matrix .columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            colname = corr_matrix.columns[i]
            corr_features.add(colname)
  return corr_features

def Mutual_Information(x_train, x_test, select_k = 10):
  # import the required functions and object.
  from sklearn.feature_selection import mutual_info_classif
  from sklearn.feature_selection import SelectKBest

  # get only the numerical features.
  numerical_x_train = x_train[x_train.select_dtypes([np.number]).columns]


  # create the SelectKBest with the mutual info strategy.
  selection = SelectKBest(mutual_info_classif, k=select_k)
  selection.fit(numerical_x_train, y_train)

  # display the retained features.
  features = x_train.columns[selection.get_support()]
  return features

def Select_Model(x_train,y_train,x_test,y_test):
  from sklearn.feature_selection import SelectFromModel
  from sklearn.ensemble import RandomForestClassifier
  # define model
  rfc = RandomForestClassifier(n_estimators=100)
  # feature extraction
  select_model = SelectFromModel(rfc)
  # fit on train set
  fit = select_model.fit(x_train, y_train)
  # transform train set
  x_train = fit.transform(x_train)
  x_test = fit.transform(x_test)
  return x_train, x_test

def PCA_Feature(x_train,x_test):
  import numpy as np
  from sklearn.decomposition import PCA
  pca = PCA(n_components=2)
  pca.fit(x_train)
  x_train = pca.transform(x_train)

def RFE_Feature(x_train,y_train,x_test,y_test):
  from sklearn.feature_selection import RFE
  from sklearn.ensemble import RandomForestClassifier
  # define model
  rfc = RandomForestClassifier(n_estimators=100)
  rfe = RFE(estimator=rfc, n_features_to_select=3)
  # fit the model
  rfe.fit(x_train, y_train)
  #transform the data
  x_train, y_train = rfe.transform(x_train, y_train)
  x_test, y_test = rfe.transform(x_test, y_test)
  return x_train,y_train,x_test,y_test

def Drop_Columns(x_train,x_test,columns):
  x_train = x_train.drop(labels=columns, axis=1, inplace=True)
  x_test = x_test.drop(labels=columns, axis=1, inplace=True)
  return x_train, x_test

def RFCaccuracy(X_train,X_test,Y_train,Y_test):
  import time
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.metrics import accuracy_score
  start = time.time()
  clf = RandomForestClassifier()
  clf.fit(X_train,Y_train)
  Y_predict = clf.predict(X_test)
  score = accuracy_score(Y_test,Y_predict)
  end = time.time()
  print("Accuracy score for Random Forest Classifier: ", score)
  print("Time estimated: ",end-start)
  return score

path='Santander_train.csv'
df = pd.read_csv(path)
df
#'''

df.info()

df.select_dtypes(include=['object']).columns

list_nan = []
for col in df.columns:
  if (df[col].isnull().sum()>0):
    list_nan.append(col)
list_nan

X = df.drop(['TARGET'],axis = 1)
y = df['TARGET']
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
#'''
'''
path='/content/drive/My Drive/Datasets/Py4DS_Lab6/Santander_train.csv'
df_train = pd.read_csv(path)
x_train = df_train.drop(['TARGET'],axis = 1)
y_train = df_train['TARGET']
path='/content/drive/My Drive/Datasets/Py4DS_Lab6/Santander_test.csv'
df_test = pd.read_csv(path)
x_train = df_test
#'''

cf = Constant_Features(x_train, x_test, threshold=0)
Drop_Columns(x_train,x_test,cf)
print(cf)
#'''

qcf = Quansi_Constant_Feature(x_train,y_train)
print(qcf)
Drop_Columns(x_train,x_test,qcf)
#'''

d_f = Dupplicate_Feature(x_train, x_test)
print(d_f)
Drop_Columns(x_train,x_test,d_f)
#'''

c_f = Correlated_Feature(x_train,x_test,threshold=0.8)
print(c_f)
Drop_Columns(x_train,x_test,c_f)
#'''

x_train_mi = x_train.copy()
y_train_mi = y_train.copy()
x_test_mi = x_test.copy()
y_test_mi = y_test.copy()
m_i = Mutual_Information(x_train_mi,x_test_mi, select_k = 10)
print(m_i)
x_train_mi = x_train_mi[m_i]
x_test_mi = x_test_mi[m_i]
#'''

x_train_sm = x_train.copy()
y_train_sm = y_train.copy()
x_test_sm = x_test.copy()
y_test_sm = y_test.copy()
Select_Model(x_train_sm,y_train_sm,x_test_sm,y_test_sm)
#'''

x_train_pca = x_train.copy()
y_train_pca = y_train.copy()
x_test_pca = x_test.copy()
y_test_pca = y_test.copy()
PCA_Feature(x_train_pca,y_train_pca)

'''
x_train_rfe = x_train.copy()
y_train_rfe = y_train.copy()
x_test_rfe = x_test.copy()
y_test_rfe = y_test.copy()
RFE_Feature(x_train_rfe,y_train_rfe,x_test_rfe,y_test_rfe)
#'''

print("Select Model")
RFCaccuracy(x_train_sm,x_test_sm,y_train_sm,y_test_sm)
print("*"*30)
print("Mutual Infomation")
RFCaccuracy(x_train_mi,x_test_mi,y_train_mi,y_test_mi)
print("*"*30)
print("PCA")
RFCaccuracy(x_train_pca,x_test_pca,y_train_pca,y_test_pca)
print("*"*30)

y.value_counts()

